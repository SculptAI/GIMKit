{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdde6801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/SIM/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/root/autodl-tmp/SIM/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-16 14:38:43 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-16 14:38:44 [utils.py:326] non-default args: {'model': '/root/autodl-tmp/SIM/exp/09101-GIM-1st/output/GIM', 'max_model_len': 10240, 'disable_log_stats': True}\n",
      "INFO 09-16 14:38:49 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 09-16 14:38:49 [__init__.py:1750] Using max model len 10240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:38:50,075\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-16 14:38:50 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 09-16 14:38:50 [__init__.py:2921] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/SIM/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-16 14:38:54 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:55 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:55 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='/root/autodl-tmp/SIM/exp/09101-GIM-1st/output/GIM', speculative_config=None, tokenizer='/root/autodl-tmp/SIM/exp/09101-GIM-1st/output/GIM', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10240, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/SIM/exp/09101-GIM-1st/output/GIM, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:56 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m WARNING 09-16 14:38:56 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:56 [gpu_model_runner.py:1953] Starting to load model /root/autodl-tmp/SIM/exp/09101-GIM-1st/output/GIM...\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:56 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:56 [cuda.py:328] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.71it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.66it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:58 [default_loader.py:262] Loading weights took 1.25 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:38:58 [gpu_model_runner.py:2007] Model loading took 7.6065 GiB and 1.393386 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:05 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/d6cdc98259/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:05 [backends.py:559] Dynamo bytecode transform time: 7.37 s\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.591 s\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:12 [monitor.py:34] torch.compile takes 7.37 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:13 [gpu_worker.py:276] Available KV cache memory: 12.11 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:14 [kv_cache_utils.py:849] GPU KV cache size: 88,192 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:14 [kv_cache_utils.py:853] Maximum concurrency for 10,240 tokens per request: 8.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 26.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:16 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 0.54 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=13858)\u001b[0;0m INFO 09-16 14:39:16 [core.py:214] init engine (profile, create kv cache, warmup model) took 18.56 seconds\n",
      "INFO 09-16 14:39:17 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from gimkit import MaskedTag\n",
    "\n",
    "\n",
    "model_path = \"/root/autodl-tmp/SIM/exp/09101-GIM-1st/output/GIM\"\n",
    "llm = LLM(model=model_path, max_model_len=10240)\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.0, max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b34a5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 144.65it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it, est. speed input: 88.64 toks/s, output: 92.51 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<|M_OUTPUT|><|MASKED id=\"m_1\"|>3 = 5\n",
      "* 2<|/MASKED|>\n",
      "<|MASKED id=\"m_2\"|>purple.\n",
      "* the increasing impact of climate change has created a greater awareness of<|/MASKED|>\n",
      "<|MASKED id=\"m_3\"|>climate change has created a greater awareness of<|/MASKED|>\n",
      "<|MASKED id=\"m_4\"|>climate change has created a greater awareness of<|/MASKED|>\n",
      "<|MASKED id=\"m_5\"|>1879<|/MASKED|>\n",
      "<|MASKED id=\"m_6\"|>1643<|/MASKED|>\n",
      "<|MASKED id=\"m_7\"|>236<|/MASKED|>\n",
      "<|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"<|M_INPUT|>Some simple examples:\n",
    "\n",
    "* 2 + {MaskedTag(desc=\"Please use English numbers\")} + 6 / 3 = 7\n",
    "* Mixing red and blue results in {MaskedTag(desc=\"Please use hex representation\")}\n",
    "* The growing impact of {MaskedTag(desc=\"five words\")} the need for accurate and reliable weather forecasting.\n",
    "* The growing impact of {MaskedTag(desc=\"six words only\")} the need for accurate and reliable weather forecasting.\n",
    "* {MaskedTag(desc=\"Einstein's birth year\")} - {MaskedTag(desc=\"Newton's birth year\")} = {MaskedTag(desc=\"number\")}\n",
    "<|/M_INPUT|>\"\"\"\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f33eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 395.43it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.81s/it, est. speed input: 27.62 toks/s, output: 92.85 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|M_OUTPUT|>\n",
      "<|MASKED id=\"m_1\"|>The probability that random.random() > random.random() holds true is the same as the probability that random.random() < random.random() holds true. This is because the two random numbers are independent and have a uniform distribution over the interval [0, 1]. Therefore, the probability of one being greater than the other is equal to the probability of one being less than the other.<|/MASKED|>\n",
      "<|MASKED id=\"m_2\"|>Therefore, the probability that random.random() > random.random() holds true is 0.5.<|/MASKED|>\n",
      "<|MASKED id=\"m_3\"|>This can be verified by running the following code:\n",
      "```\n",
      "import random\n",
      "count = 0\n",
      "n = 1000000\n",
      "for i in range(n):\n",
      "    if random.random() > random.random():\n",
      "        count += 1\n",
      "print(count / n)\n",
      "```\n",
      "This will output approximately 0.5.<|/MASKED|>\n",
      "<|MASKED id=\"m_4\"|>Therefore, the answer is 0.5.<|/MASKED|>\n",
      "<|MASKED id=\"m_5\"|>This is because the two random numbers are independent and have a uniform distribution over the interval [0, 1]. Therefore, the probability of one being greater than the other is equal to the probability of one being less than the other.<|/MASKED|>\n",
      "<|MASKED id=\"m_6\"|>This can be verified by running the following code:\n",
      "```\n",
      "import random\n",
      "count = 0\n",
      "n = 1000000\n",
      "for i in range(n):\n",
      "    if random.random() > random.random():\n",
      "        count += 1\n",
      "print(count / n)\n",
      "```\n",
      "This will output approximately 0.5.<|/MASKED|>\n",
      "<|MASKED id=\"m_7\"|>Therefore, the answer is 0.5.<|/MASKED|>\n",
      "<|MASKED id=\"m_8\"|>This is because the two random numbers are independent and have a uniform distribution over the interval [0, 1]. Therefore, the probability of one being greater than the other is equal to the probability of one being less than the other.<|/MASKED|>\n",
      "<|MASKED id=\"m_9\"|>This can be verified by running the following code:\n",
      "```\n",
      "import random\n",
      "count = 0\n",
      "n = 1000000\n",
      "for i in range(n):\n",
      "    if random.random() > random.random():\n",
      "        count += 1\n",
      "print(count / n)\n",
      "```\n",
      "This will output approximately 0.5.<|/MASKED|>\n",
      "<|MASKED id=\"m_10\"|>Therefore, the answer is 0.5.<|/MASKED|>\n",
      "<|MASKED id=\"m_11\"|>0.5<|/MASKED|>\n",
      "<|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"<|M_INPUT|>\n",
    "From the perspective of floating-point storage, what is the probability that random.random() > random.random() holds true?\n",
    "\n",
    "Let's reason step by step:\n",
    "1. {MaskedTag()}\n",
    "2. {MaskedTag()}\n",
    "3. {MaskedTag()}\n",
    "4. {MaskedTag()}\n",
    "5. {MaskedTag()}\n",
    "6. {MaskedTag()}\n",
    "7. {MaskedTag()}\n",
    "8. {MaskedTag()}\n",
    "9. {MaskedTag()}\n",
    "10. {MaskedTag()}\n",
    "\n",
    "Final Answer: {MaskedTag()}\n",
    "<|/M_INPUT|>\"\"\"\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b520c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 466.34it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it, est. speed input: 63.45 toks/s, output: 93.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|M_OUTPUT|>\n",
      "<|MASKED id=\"m_1\"|>24 = 2 x 12\n",
      "12 = 6 x 2\n",
      "2 = 6 / 3\n",
      "Therefore, 24 = 2 x (6 x 2) x (6 / 3) = 2 x 12 x 2 = 48<|/MASKED|><|MASKED id=\"m_2\"|>x<|/MASKED|><|MASKED id=\"m_3\"|>x<|/MASKED|><|MASKED id=\"m_4\"|>/<|/MASKED|><|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"<|M_INPUT|>\n",
    "Your Reasoning for the 24 puzzle below: {MaskedTag(desc=\"step by step reasoning; think carefully\")}\n",
    "\n",
    "2{MaskedTag(desc=\"math operator\")}6{MaskedTag(desc=\"math operator\")}6{MaskedTag(desc=\"math operator\")}2=24\n",
    "<|/M_INPUT|>\"\"\"\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e0e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 252.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it, est. speed input: 166.73 toks/s, output: 93.21 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|M_OUTPUT|>\n",
      "|<|MASKED id=\"m_1\"|>Smart Home - Lighting Control<|/MASKED|>\n",
      "|<|MASKED id=\"m_2\"|>Deploy IoT devices to monitor soil moisture, temperature, and nutrient levels, enabling precise irrigation and crop management. |\n",
      "|<|MASKED id=\"m_3\"|>Smart Factory - Predictive Maintenance<|/MASKED|>\n",
      "|<|MASKED id=\"m_4\"|>Smart Agriculture - Crop Monitoring<|/MASKED|>\n",
      "|<|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"<|M_INPUT|>\n",
    "| Application Scenario     | Description                                                   |\n",
    "|--------------------------|-------------------------------------------------------------|\n",
    "| {MaskedTag()}       | Achieve automated lighting control through protocols like Wi-Fi or Zigbee, enhancing convenience and energy efficiency. |\n",
    "| Smart City - Traffic Management | Optimize traffic flow using 5G networks and AI video analysis, improving traffic safety and efficiency. |\n",
    "| Industrial IoT - Maintenance    | Use IIoT platforms and machine learning for predictive maintenance, reducing downtime and maintenance costs. |\n",
    "| Agriculture - Precision Farming  | {MaskedTag()} |\n",
    "| Healthcare - Remote Monitoring   | Real-time monitoring of patient health status through wearable devices, supporting telemedicine and personalized treatment. |\n",
    "| {MaskedTag(desc=\"Scenario\")} | {MaskedTag(desc=\"Description\")} |\n",
    "| Environmental Protection - Air Quality | Deploy miniature air quality sensors to monitor and warn of air pollution events, protecting public health. |\n",
    "<|/M_INPUT|>\"\"\"\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd78a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 432.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 103.13 toks/s, output: 94.12 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|M_OUTPUT|>\n",
      "<|MASKED id=\"m_1\"|>[\"topological_sort\", \"graph\", \"topological_sorting\", \"graph_algorithms\"]\n",
      "<|/MASKED|><|MASKED id=\"m_2\"|>O(V + E)<|/MASKED|><|MASKED id=\"m_3\"|>O(V + E)<|/MASKED|>\n",
      "<|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"<|M_INPUT|>\n",
    "Complete the algorithm profile in TOML:\n",
    "\n",
    "[algorithm_profile]\n",
    "name = \"Topological Sort\"\n",
    "type = {MaskedTag(desc=\"list[str] type; 4 items\")}\n",
    "performance.time_complexity: {MaskedTag(desc=\"str type; use LaTeX expression\")},\n",
    "performance.space_complexity: {MaskedTag(desc=\"str type; use LaTeX expression\")}\n",
    "<|/M_INPUT|>\"\"\"\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839c4ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 214.02it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.99s/it, est. speed input: 70.83 toks/s, output: 92.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "<|M_OUTPUT|>\n",
      "<|MASKED id=\"m_1\"|>Romeo<|/MASKED|><|MASKED id=\"m_2\"|>is a character in<|/MASKED|><|MASKED id=\"m_3\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_4\"|>Juliet<|/MASKED|><|MASKED id=\"m_5\"|>is a character in<|/MASKED|><|MASKED id=\"m_6\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_7\"|>Romeo<|/MASKED|><|MASKED id=\"m_8\"|>is a character in<|/MASKED|><|MASKED id=\"m_9\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_10\"|>Romeo<|/MASKED|><|MASKED id=\"m_11\"|>is a character in<|/MASKED|><|MASKED id=\"m_12\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_13\"|>Juliet<|/MASKED|><|MASKED id=\"m_14\"|>is a character in<|/MASKED|><|MASKED id=\"m_15\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_16\"|>Romeo<|/MASKED|><|MASKED id=\"m_17\"|>is a character in<|/MASKED|><|MASKED id=\"m_18\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_19\"|>Juliet<|/MASKED|><|MASKED id=\"m_20\"|>is a character in<|/MASKED|><|MASKED id=\"m_21\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_22\"|>Romeo<|/MASKED|><|MASKED id=\"m_23\"|>is a character in<|/MASKED|><|MASKED id=\"m_24\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_25\"|>Juliet<|/MASKED|><|MASKED id=\"m_26\"|>is a character in<|/MASKED|><|MASKED id=\"m_27\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_28\"|>Romeo<|/MASKED|><|MASKED id=\"m_29\"|>is a character in<|/MASKED|><|MASKED id=\"m_30\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|MASKED id=\"m_31\"|>Juliet<|/MASKED|><|MASKED id=\"m_32\"|>is a character in<|/MASKED|><|MASKED id=\"m_33\"|>Romeo and Juliet<|/MASKED|>\n",
      "<|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"<|M_INPUT|>\n",
    "## Content\n",
    "\n",
    "We were both young when I first saw you\n",
    "I close my eyes and the flashback starts\n",
    "I'm standin' there\n",
    "On a balcony in summer air\n",
    "See the lights, see the party, the ball gowns\n",
    "See you make your way through the crowd\n",
    "And say, \"Hello\"\n",
    "Little did I know\n",
    "That you were Romeo, you were throwin' pebbles\n",
    "And my daddy said, \"Stay away from Juliet\"\n",
    "And I was cryin' on the staircase\n",
    "Beggin' you, \"Please don't go, \" and I said\n",
    "Romeo, take me somewhere we can be alone\n",
    "I'll be waiting, all there's left to do is run\n",
    "You'll be the prince and I'll be the princess\n",
    "It's a love story, baby, just say, \"Yes\"\n",
    "\n",
    "## Extraction\n",
    "\n",
    "Extract knowledge graph triplets (head, relation, tail) from the content.\n",
    "\n",
    "1. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "2. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "3. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "4. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "5. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "6. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "7. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "8. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "9. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "10. ({MaskedTag()}, {MaskedTag()}, {MaskedTag()})\n",
    "<|/M_INPUT|>\"\"\"\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98c30a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 652.91it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.19s/it, est. speed input: 84.72 toks/s, output: 92.87 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "<|M_OUTPUT|><|MASKED id=\"m_1\"|>淀粉和纤维素的化学式都是（C<sub>6</sub>H<sub>10</sub>O<sub>5</sub>）<sub>n</sub>，但n值不同，所以它们不是同分异构体，故B错误；<|/MASKED|>\n",
      "<|MASKED id=\"m_2\"|>淀粉和纤维素都属于多糖，水解的最终产物都是葡萄糖，故A正确；<|/MASKED|>\n",
      "<|MASKED id=\"m_3\"|>淀粉和纤维素都属于多糖，但淀粉不溶于水，纤维素微溶于水，故C错误；<|/MASKED|>\n",
      "<|MASKED id=\"m_4\"|>淀粉和纤维素的化学式都是（C<sub>6</sub>H<sub>10</sub>O<sub>5</sub>）<sub>n</sub>，但n值不同，所以它们不是同分异构体，故D错误；<|/MASKED|>\n",
      "<|MASKED id=\"m_5\"|>综上所述，只有A正确。<|/MASKED|>\n",
      "<|MASKED id=\"m_6\"|>A<|/MASKED|>\n",
      "<|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"<|M_INPUT|>对于淀粉和纤维素两种物质，下列说法正确的是____\n",
    "A. 两者都能水解，且水解的最终产物相同\n",
    "B. 两者含C、H、O三种元素的质量分数相同，且互为同分异构体\n",
    "C. 它们都属于糖类，且都是溶于水的高分子化合物\n",
    "D. 都可 用$\\\\left ( C_6H_6O_5\\\\right )_n$表示，但淀粉能发生银镜反应，而纤维素不能\n",
    "\n",
    "Let's think step by step.\n",
    "1. {MaskedTag(id=1, desc=\"step\")}\n",
    "2. {MaskedTag(id=2, desc=\"step\")}\n",
    "3. {MaskedTag(id=3, desc=\"step\")}\n",
    "4. {MaskedTag(id=4, desc=\"step\")}\n",
    "5. {MaskedTag(id=5, desc=\"step\")}\n",
    "所以答案是：{MaskedTag(id=6, desc=\"选项对应的文本\")}\n",
    "<|/M_INPUT|>\"\"\"\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c00cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 571.28it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 41.65 toks/s, output: 91.94 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|M_OUTPUT|>\n",
      "<|MASKED id=\"m_1\"|>word_count<|/MASKED|><|MASKED id=\"m_2\"|>\n",
      "    word_count = {}\n",
      "    with open(filename, 'r') as f:\n",
      "        for line in f:\n",
      "            for word in line.split():\n",
      "                word = word.lower()\n",
      "                if word not in word_count:\n",
      "                    word_count[word] = 0\n",
      "                word_count[word] += 1\n",
      "    return word_count<|/MASKED|>\n",
      "<|/M_OUTPUT|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example from InCoder (Fried, et al. 2022)\n",
    "question = f'''<|M_INPUT|>\n",
    "def {MaskedTag()}:\n",
    "    \"\"\" Count the number of occurrences of each word in the file. \"\"\"\n",
    "{MaskedTag()}\n",
    "<|/M_INPUT|>'''\n",
    "\n",
    "print(llm.generate([question], sampling_params)[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simfill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
